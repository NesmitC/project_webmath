# app/assistant.py

import os
import re
import requests
import time
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
import pandas as pd

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
HF_CACHE_DIR = os.path.join("models", "embeddings")
os.makedirs(HF_CACHE_DIR, exist_ok=True)

_VECTOR_STORE_PATH = "faiss_index_webmath"
CSV_URL = "https://docs.google.com/spreadsheets/d/1dQxvFLhajJOvdCM2rdqTMeJhy49cOVBQKfRH_3i0IRw/export?format=csv&gid=0"

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ---
_embeddings = None
_db = None
_retriever = None
_hard_cases = {}

# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ ---
def load_hard_cases():
    global _hard_cases
    try:
        df = pd.read_csv(CSV_URL)
        print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        for _, row in df.iterrows():
            triggers = str(row["trigger"]).lower().split("|")
            for trigger in triggers:
                trigger = trigger.strip()
                if trigger:
                    _hard_cases[trigger] = {
                        "answer": row["answer"],
                        "rule": row["rule"]
                    }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")

# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Google Docs ---
def load_document_text(url: str) -> str:
    url = url.strip().split('/edit')[0]
    match_ = re.search(r'/document/d/([a-zA-Z0-9-_]+)', url)
    if not match_:
        raise ValueError('Invalid Google Docs URL')
    doc_id = match_.group(1).strip()
    export_url = f'https://docs.google.com/document/d/{doc_id}/export?format=txt'
    response = requests.get(export_url)
    response.raise_for_status()
    return response.text

# --- –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---
def get_retriever():
    global _retriever, _embeddings, _db

    if _retriever is not None:
        return _retriever

    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")

    max_retries = 3
    for attempt in range(1, max_retries + 1):
        try:
            # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π safetensors
            _embeddings = HuggingFaceEmbeddings(
#                model_name="sentence-transformers/all-MiniLM-L6-v2",
#                model_name="sentence-transformers/paraphrase-MiniLM-L3-v2",
                model_name="./models/embeddings/sentence-transformers__all-MiniLM-L6-v2",
                cache_folder=HF_CACHE_DIR,
                encode_kwargs={
                    "device": "cpu"
                }
            )
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            if attempt == max_retries:
                raise
            time.sleep(5)

            test_emb = _embeddings.embed_query("–ø—Ä–∏–≤–µ—Ç")
            print(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç, —Ä–∞–∑–º–µ—Ä: {len(test_emb)}")

    # --- –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS –±–∞–∑—ã ---
    if os.path.exists(_VECTOR_STORE_PATH):
        print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ FAISS-–∏–Ω–¥–µ–∫—Å–∞...")
        _db = FAISS.load_local(
            _VECTOR_STORE_PATH,
            _embeddings,
            allow_dangerous_deserialization=True
        )
    else:
        print("üÜï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS-–∏–Ω–¥–µ–∫—Å–∞...")
        try:
            raw_data = load_document_text('https://docs.google.com/document/d/1CcW-xOVPIPUZaiMxJsl5SmCKyebfhIYwmkz1ONH_neI/edit?usp=sharing')
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}")
            raw_data = ""

        definition = "–°–ø—Ä—è–∂–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≥–ª–∞–≥–æ–ª–æ–≤ –ø–æ –ª–∏—Ü–∞–º –∏ —á–∏—Å–ª–∞–º. –í —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–≤–∞ —Å–ø—Ä—è–∂–µ–Ω–∏—è: –ø–µ—Ä–≤–æ–µ –∏ –≤—Ç–æ—Ä–æ–µ."
        cleaned_data = definition + "\n\n" + raw_data

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)
        texts = text_splitter.split_text(cleaned_data)
        docs = [Document(page_content=txt) for txt in texts]

        _db = FAISS.from_documents(docs, _embeddings)
        _db.save_local(_VECTOR_STORE_PATH)
        print("‚úÖ FAISS-–∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ª–æ–∫–∞–ª—å–Ω–æ.")

    _retriever = _db.as_retriever(search_kwargs={"k": 2})
    print("‚úÖ –ú–æ–¥–µ–ª—å –∏ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≥–æ—Ç–æ–≤—ã.")
    return _retriever

# --- DeepSeek API ---
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
# if not DEEPSEEK_API_KEY:
  #  raise EnvironmentError("DEEPSEEK_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

DEEPSEEK_URL = "https://api.deepseek.com/v1/chat/completions"
HEADERS = {
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
    "Content-Type": "application/json"
}

def deepseek_complete(prompt: str) -> str:
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 200
    }
    try:
        response = requests.post(DEEPSEEK_URL, json=payload, headers=HEADERS)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ DeepSeek: {str(e)}"

# --- –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ---
def ask_teacher(question: str):
    question = question.strip()
    if not question:
        return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å."

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    q_lower = question.lower()
    for trigger in _hard_cases:
        if trigger in q_lower:
            case = _hard_cases[trigger]
            answer = case["answer"]
            if case["rule"]:
                answer += f" –ü—Ä–∞–≤–∏–ª–æ: {case['rule']}"
            return answer

    # 2. –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    try:
        retriever = get_retriever()
        relevant_docs = retriever.get_relevant_documents(question)

        if not relevant_docs:
            return "–í–∞—à –≤–æ–ø—Ä–æ—Å –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–π —Å–ª—É–∂–±–µ."
        
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        prompt = f"""
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —É—á–∏—Ç–µ–ª—å —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Ç–æ—á–Ω–æ –∏ —Ç–æ–ª—å–∫–æ –ø–æ —Ñ–∞–∫—Ç–∞–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∫—Ä–∞—Ç–∫–æ –ø–æ—è—Å–Ω–∏, –ü–û–ß–ï–ú–£ —Ç–∞–∫ –ø–∏—à–µ—Ç—Å—è, –∞ –Ω–µ –∏–Ω–∞—á–µ.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
2. –ë—É–¥—å –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª–µ–Ω, –Ω–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∑–¥–æ—Ä–æ–≤–∞–π—Å—è, –ø–æ—Ç–æ–º –ø—Ä–µ–¥–ª–∞–≥–∞–π –ø–æ–º–æ—â—å,
–Ω–µ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –≤ —Å–∫–æ–±–∫–∞—Ö. –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫.
3. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—ç—Ç–∏—á–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è, –º–∞—Ç, —Ç–≤–µ—Ä–¥–æ —Å–∫–∞–∂–∏ –æ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ—Å—Ç–∏ —Ç–∞–∫–æ–≥–æ —Ä–µ—á–µ–≤–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è,
—ç—Ç–æ –¥–µ–ª–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞ —É–±–æ–≥–∏–º, –Ω–µ–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–º.
4. –û—Ç–≤–µ—Ç ‚Äî 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–≤–µ—Ç, –ø–æ—Ç–æ–º –ö–†–ê–¢–ö–û–ï –ø—Ä–∞–≤–∏–ª–æ –∏–ª–∏ –ø—Ä–∏—á–∏–Ω–∞.
5. –ù–µ —É–ø–æ–º–∏–Ω–∞–π "—Ä–∞–∑–¥–µ–ª", "—á–∞–Ω–∫", "–∑–∞–≥–æ–ª–æ–≤–æ–∫".
6. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å –æ—à–∏–±–∫–æ–π ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–ü—Ä–∞–≤–∏–ª—å–Ω–æ: ...¬ª –∏ –æ–±—ä—è—Å–Ω–∏ –ø–æ—á–µ–º—É.
7. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω ‚Äî –ø–æ–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω–∏—Ç—å.
8. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏: ¬´–í–∞—à –≤–æ–ø—Ä–æ—Å –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–π —Å–ª—É–∂–±–µ.¬ª

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:
"""
        return deepseek_complete(prompt)
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ ---
if __name__ == "__main__":
    load_hard_cases()
    print("‚è≥ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ...")
    get_retriever()
    print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")