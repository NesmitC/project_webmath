# app/neuro_method.py

import os
import fitz  # PyMuPDF
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
# –ü—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞: project_webmath/data/methodist
PDF_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "methodist")
METHODIST_INDEX_PATH = "faiss_index_methodist"

# –û–±—â–∞—è –º–æ–¥–µ–ª—å (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ, —á—Ç–æ –∏ —É—á–∏—Ç–µ–ª—è)
EMBEDDINGS_MODEL_PATH = "./models/embeddings/sentence-transformers__all-MiniLM-L6-v2"

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ---
_embeddings = None
_db = None
_retriever = None

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ---
def get_embeddings():
    global _embeddings
    if _embeddings is None:
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –º–µ—Ç–æ–¥–∏—Å—Ç–∞...")
        _embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDINGS_MODEL_PATH,
            encode_kwargs={"device": "cpu"}
        )
    return _embeddings

# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF ---
def extract_text_from_pdfs(pdf_dir: str) -> str:
    full_text = ""
    if not os.path.exists(pdf_dir):
        print(f"‚ùå –ü–∞–ø–∫–∞ {pdf_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return full_text

    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
    if not pdf_files:
        print("‚ö†Ô∏è –í –ø–∞–ø–∫–µ pdfs/ –Ω–µ—Ç PDF-—Ñ–∞–π–ª–æ–≤")

    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_dir, pdf_file)
        try:
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text() + "\n"
            doc.close()
            print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {pdf_file}")
            full_text += f"\n\n--- –î–æ–∫—É–º–µ–Ω—Ç: {pdf_file} ---\n\n{text}"
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {pdf_file}: {e}")
    return full_text

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---
def get_retriever():
    global _retriever, _db

    if _retriever is not None:
        return _retriever

    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞-–º–µ—Ç–æ–¥–∏—Å—Ç–∞...")

    embeddings = get_embeddings()

    if os.path.exists(METHODIST_INDEX_PATH):
        print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –º–µ—Ç–æ–¥–∏—Å—Ç–∞...")
        _db = FAISS.load_local(
            METHODIST_INDEX_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
    else:
        print("üÜï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS-–∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –º–µ—Ç–æ–¥–∏—Å—Ç–∞...")
        raw_text = extract_text_from_pdfs(PDF_DIR)

        if not raw_text.strip():
            print("‚ö†Ô∏è –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF ‚Äî —Å–æ–∑–¥–∞–Ω–∞ –ø—É—Å—Ç–∞—è –±–∞–∑–∞")
            raw_text = "–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é, –§–ì–û–°, –∫–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º –∏ –¥–æ–ø—É—Å–∫—É –∫ —ç–∫–∑–∞–º–µ–Ω–∞–º –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        texts = text_splitter.split_text(raw_text)
        docs = [Document(page_content=txt) for txt in texts]

        _db = FAISS.from_documents(docs, embeddings)
        _db.save_local(METHODIST_INDEX_PATH)
        print("‚úÖ –ò–Ω–¥–µ–∫—Å –º–µ—Ç–æ–¥–∏—Å—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ª–æ–∫–∞–ª—å–Ω–æ.")

    _retriever = _db.as_retriever(search_kwargs={"k": 2})
    print("‚úÖ –ù–µ–π—Ä–æ–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-–º–µ—Ç–æ–¥–∏—Å—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")
    return _retriever

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ---
def ask_methodist(question: str) -> str:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–æ–ø—Ä–æ—Å, –∏—â–µ—Ç –≤ PDF-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç.
    –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ —É—á–∏—Ç–µ–ª—è.
    """
    question = question.strip().lower()
    if not question:
        return None

    # üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –ø–æ–Ω—è—Ç–Ω–æ, —á—Ç–æ –≤–æ–ø—Ä–æ—Å –∫ –º–µ—Ç–æ–¥–∏—Å—Ç—É
    trigger_words = [
        "—ç–∫–∑–∞–º–µ–Ω", "–æ–≥—ç", "–µ–≥—ç", "—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ", "–¥–æ–ø—É—Å–∫", "–¥–æ–∫—É–º–µ–Ω—Ç", "—Ñ–≥–æ—Å", "–∫–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä",
        "–ø—Ä–æ–≥—Ä–∞–º–º–∞", "–∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è", "–∑–∞—è–≤–ª–µ–Ω–∏–µ", "—Å—Ä–æ–∫–∏", "–¥–∞—Ç–∞", "—Ä–µ–≥–ª–∞–º–µ–Ω—Ç", "–ø—Ä–æ—Ö–æ–¥–Ω–æ–π",
        "—Ñ–æ—Ä–º–∞", "–±–ª–∞–Ω–∫", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "–ø—Ä–∞–≤–∏–ª–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è", "–ø—Ä–æ–≤–µ—Ä—è—é—â–∏–π"
    ]

    if not any(word in question for word in trigger_words):
        return None  # –ù–µ –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å ‚Üí –ø–µ—Ä–µ–¥–∞—ë–º —É—á–∏—Ç–µ–ª—é

    try:
        retriever = get_retriever()
        relevant_docs = retriever.get_relevant_documents(question)
        if not relevant_docs:
            return "–ù–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ–∫–∞ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –≤ –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏."

        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ DeepSeek, —á—Ç–æ –∏ —É —É—á–∏—Ç–µ–ª—è
        from app.assistant import deepseek_complete

        prompt = f"""
–¢—ã ‚Äî –Ω–µ–π—Ä–æ–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-–º–µ—Ç–æ–¥–∏—Å—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Ç–æ—á–Ω–æ –∏ —Ç–æ–ª—å–∫–æ –ø–æ —Ñ–∞–∫—Ç–∞–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º.
2. –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
3. –ù–µ —É–ø–æ–º–∏–Ω–∞–π "—á–∞–Ω–∫", "—Ä–∞–∑–¥–µ–ª".
4. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω ‚Äî –ø–æ–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω–∏—Ç—å.
5. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏: ¬´–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.¬ª

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:
"""
        return deepseek_complete(prompt)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –º–µ—Ç–æ–¥–∏—Å—Ç–∞: {str(e)}"